# Open LLMs

These LLMs are all licensed for commercial use (e.g., Apache 2.0). Contributions and corrections welcome!


- T5
  - Checkpoints: [T5 & Flan-T5](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints), [Flan-T5-xxl (HF)](https://huggingface.co/google/flan-t5-xxl)
  - Paper/blog: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)
  - Size: 60M - 11B
  - Licence: Apache 2.0

  
- UL2
  - Checkpoints: [UL2 & Flan-UL2](https://github.com/google-research/google-research/tree/master/ul2#checkpoints), [Flan-UL2 (HF)](https://huggingface.co/google/flan-ul2)
  - Paper/blog: [UL2 20B: An Open Source Unified Language Learner](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)
  - Size: 20B
  - Licence: Apache 2.0


- Cerebras-GPT
  - Checkpoints: [Cerebras-GPT](https://huggingface.co/cerebras)
  - Paper/blog: [Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/), [Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster](https://arxiv.org/abs/2304.03208)
  - Size: 111M - 13B
  - Licence: Apache 2.0


- Pythia
  - Checkpoints: [pythia 70M - 12B](https://github.com/EleutherAI/pythia)
  - Paper/blog: [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)
  - Size: 70M - 12B
  - License: Apache 2.0


- Dolly
  - Checkpoints: [dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)
  - Paper/blog: [Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)
  - Size: 3B, 7B, 12B
  - Licence: MIT

- RWKV
  - Checkpoints: [RWKV, ChatRWKV](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v)
  - Paper/blog: [The RWKV Language Model (and my LM tricks)](https://github.com/BlinkDL/RWKV-LM)
  - Size: 100M - 14B
  - Licence: Apache 2.0


- GPT-J-6B
  - Checkpoints: [GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b), [GPT4All-J](https://github.com/nomic-ai/gpt4all#raw-model)
  - Paper/blog: [GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)
  - Size: 6B
  - Licence: Apache 2.0

  
- StableLM
  - Checkpoints: [StableLM](https://github.com/stability-AI/stableLM/#models)
  - Paper/blog: [Stability AI Launches the First of its StableLM Suite of Language Models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)
  - Size: 3B - 65B
  - Licence: CC BY-SA-4.0 license


- StarCoder
  - Checkpoints: [starcoder](https://huggingface.co/bigcode/starcoder)
  - Paper/blog: [StarCoder: A State-of-the-Art LLM for Code](https://huggingface.co/blog/starcoder), [StarCoder: May the source be with you!](https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view)
  - Size: 15B
  - Licence: [BigCode OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement)


- MPT-7B
  - Checkpoints: [MPT-7B base, instruct, etc](https://github.com/mosaicml/llm-foundry#mpt)
  - Paper/blog: [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Size: 7B
  - Licence: Apache 2.0 for base and storywriter 


- Replit Code-v1-3b
  - Checkpoints: https://huggingface.co/replit/replit-code-v1-3b
  - Paper/blog: https://www.latent.space/p/reza-shabani#details
  - Size: 2.7B
  - License: Creative Commons license (CC BY-SA-4.0)
---

### Want to contribute? Just add to the above with the following

- Name of model
  - Checkpoints:
  - Paper/blog: 
  - Size: 
  - Licence:

--- 

### Improvements

- Add context size?
- Add (links to) eval benchmarks? 
- Update to use table format
